{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOAMLzFgHHAJSvEnv1KGC+v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Configura tokenização 🔣"],"metadata":{"id":"tXf1cakhGfg0"}},{"cell_type":"code","source":["pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aq51a60m8iG4","executionInfo":{"status":"ok","timestamp":1739716365950,"user_tz":180,"elapsed":12257,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}},"outputId":"6466ddc4-f243-4182-af11-5e740f53faf1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n"]}]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","# Função texto -> token\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n","    return encoded_tensor\n","\n","# Função token -> texto\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0)\n","    return tokenizer.decode(flat.tolist())"],"metadata":{"id":"xcuvORDZ_aWy","executionInfo":{"status":"ok","timestamp":1739716370202,"user_tz":180,"elapsed":3062,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Baixa e configura modelo *GPT* (livro base) ↪"],"metadata":{"id":"f5svMKYcDFt1"}},{"cell_type":"markdown","source":["Baixa arquivos relacionados ao modelo do livro base"],"metadata":{"id":"g8vZ0oaXDN7D"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"BH2sYMzK2HAh","executionInfo":{"status":"ok","timestamp":1739716374363,"user_tz":180,"elapsed":1157,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}}},"outputs":[],"source":["import os\n","\n","# Baixa o arquivo gpt_download.py do livro\n","if not os.path.exists(\"gpt_download.py\"):\n","  url = \"https://raw.githubusercontent.com/rasbt/\"\\\n","        \"LLMs-from-scratch/main/ch05/\"\\\n","        \"01_main-chapter-code/gpt_download.py\"\n","  cmd = ! wget {url}\n","\n","# Baixa o modelo do GPT do livro\n","if not os.path.exists(\"previous_chapters.py\"):\n","  url = \"https://raw.githubusercontent.com/rasbt/\"\\\n","        \"LLMs-from-scratch/main/ch05/\"\\\n","        \"01_main-chapter-code/previous_chapters.py\"\n","  cmd = ! wget {url}\n"]},{"cell_type":"markdown","source":["Baixa modelo GPT2"],"metadata":{"id":"dcnPu90ADgZ-"}},{"cell_type":"code","source":["from gpt_download import download_and_load_gpt2\n","\n","# Baixa modelo\n","settings, params = download_and_load_gpt2(\n","    model_size=\"124M\", models_dir=\"gpt2\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrGXr7EV3xhi","executionInfo":{"status":"ok","timestamp":1739716415857,"user_tz":180,"elapsed":38787,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}},"outputId":"7d913fc6-f833-430a-c1a4-f8d7d4314080"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 138kiB/s]\n","encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.47MiB/s]\n","hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 78.8kiB/s]\n","model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:29<00:00, 16.8MiB/s]\n","model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 8.15MiB/s]\n","model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.72MiB/s]\n","vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.79MiB/s]\n"]}]},{"cell_type":"markdown","source":["Configuração do modelo"],"metadata":{"id":"35NKO_IRHsOh"}},{"cell_type":"code","source":["import torch\n","from previous_chapters import GPTModel\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Tamanho do vocabulário\n","    \"context_length\": 256, # Tamanho do contexto reduzido (original: 1024)\n","    \"emb_dim\": 768,        # Dimensão do embedding\n","    \"n_heads\": 12,         # Número de cabeças de atenção\n","    \"n_layers\": 12,        # Número de camadas\n","    \"drop_rate\": 0.1,      # Taxa de dropout\n","    \"qkv_bias\": False      # Viés de consulta-chave-valor\n","}\n","\n","# Configuração de modelos disponíveis\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","# Termina de configura modelo (no nosso caso o \"gpt2-small (124M)\")\n","model_name = \"gpt2-small (124M)\"\n","NEW_CONFIG = GPT_CONFIG_124M\n","NEW_CONFIG.update(model_configs[model_name])\n","NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n","\n","gpt = GPTModel(NEW_CONFIG)\n","gpt.eval();"],"metadata":{"id":"z72D4NIN5XSg","executionInfo":{"status":"ok","timestamp":1739716426263,"user_tz":180,"elapsed":7110,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Carrega pesos do GPT com os devidos ajustes"],"metadata":{"id":"QzhcxS1ZFT5P"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Função auxiliar de ajuste de tensores\n","def assign(left, right):\n","    if left.shape != right.shape:\n","        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n","    return torch.nn.Parameter(torch.tensor(right))\n","\n","# Função que carrega os pesos do GPT com ajustes\n","def load_weights_into_gpt(gpt, params):\n","    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n","    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n","\n","    for b in range(len(params[\"blocks\"])):\n","        q_w, k_w, v_w = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.weight = assign(\n","            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n","        gpt.trf_blocks[b].att.W_key.weight = assign(\n","            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n","        gpt.trf_blocks[b].att.W_value.weight = assign(\n","            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n","\n","        q_b, k_b, v_b = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.bias = assign(\n","            gpt.trf_blocks[b].att.W_query.bias, q_b)\n","        gpt.trf_blocks[b].att.W_key.bias = assign(\n","            gpt.trf_blocks[b].att.W_key.bias, k_b)\n","        gpt.trf_blocks[b].att.W_value.bias = assign(\n","            gpt.trf_blocks[b].att.W_value.bias, v_b)\n","\n","        gpt.trf_blocks[b].att.out_proj.weight = assign(\n","            gpt.trf_blocks[b].att.out_proj.weight,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].att.out_proj.bias = assign(\n","            gpt.trf_blocks[b].att.out_proj.bias,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[0].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[0].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n","        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[2].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[2].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].norm1.scale = assign(\n","            gpt.trf_blocks[b].norm1.scale,\n","            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n","        gpt.trf_blocks[b].norm1.shift = assign(\n","            gpt.trf_blocks[b].norm1.shift,\n","            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n","        gpt.trf_blocks[b].norm2.scale = assign(\n","            gpt.trf_blocks[b].norm2.scale,\n","            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n","        gpt.trf_blocks[b].norm2.shift = assign(\n","            gpt.trf_blocks[b].norm2.shift,\n","            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n","\n","    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n","    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n","    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n","\n","# Carrega os pesos\n","load_weights_into_gpt(gpt, params)\n","\n","# Usa o cuda se possível (otimização)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","gpt.to(device);"],"metadata":{"id":"T1mlheJc-EMt","executionInfo":{"status":"ok","timestamp":1739716437074,"user_tz":180,"elapsed":1079,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Geração de texto 🖊"],"metadata":{"id":"JVnfuVzqHcC1"}},{"cell_type":"code","source":["# Função de geração de texto\n","def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # Filtra os logits com amostragem top_k\n","        if top_k is not None:\n","            # Mantém apenas os top_k valores\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val,\n","                          torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        # Aplica o escalonamento de temperatura\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Aplica softmax para obter as probabilidades\n","            probs = torch.softmax(logits, dim=-1)\n","\n","            # Amostra a partir da distribuição\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","\n","        # Caso contrário, o mesmo de antes: obtém o idx da entrada do\n","        # vocabulário com o valor de logits mais alto\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n","\n","        # Para de gerar precocemente se o token de fim de sequência for\n","        # encontrado e eos_id for especificado\n","        if idx_next == eos_id:\n","            break\n","\n","        # Mesmo de antes: anexa o índice amostrado à sequência em execução\n","        idx = torch.cat((idx, idx_next), dim=1)\n","\n","    return idx"],"metadata":{"id":"M4r1ak9L-3lN","executionInfo":{"status":"ok","timestamp":1739716440847,"user_tz":180,"elapsed":251,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Gera texto"],"metadata":{"id":"afYhsubfH3jP"}},{"cell_type":"code","source":["# Seed escolhida\n","torch.manual_seed(78)\n","\n","token_ids = generate(\n","    model=gpt,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n","    max_new_tokens=50,\n","    context_size=NEW_CONFIG[\"context_length\"],\n","    top_k=50,\n","    temperature=1.5\n",")\n","\n","print(f\"Texto de saída:\\n\\n{token_ids_to_text(token_ids, tokenizer)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-TuG04D-s-6","executionInfo":{"status":"ok","timestamp":1739716458271,"user_tz":180,"elapsed":12916,"user":{"displayName":"Bruno Berto","userId":"06526414926027817953"}},"outputId":"950f05e3-c9af-47c4-d3a1-accd2adf4444"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Texto de saída:\n","\n","Every effort moves you\n","\n","as one, like God and earth\n","\n","as waters, on\n","\n","a plane to move. Each\n","\n","spirit makes every motion\n","\n","the entire physical force of existence. No\n","\n","place is the centre where the movements unfold -\n"]}]}]}